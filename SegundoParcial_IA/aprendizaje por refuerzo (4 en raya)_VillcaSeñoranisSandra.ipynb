{"cells":[{"cell_type":"markdown","metadata":{"id":"OlxulJVRVg2n"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/axr/blob/master/axr/00_intro.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"mYaNMwi2Vg2r"},"source":["# Pregunta 2. Aprendizaje por refuerzo aplicado a 4 en raya"]},{"cell_type":"markdown","source":["INTERACCIÓN: HAY UN MUNDO O UN ENTORNO DONDE LOS ALGORITMOS DE AxR (AGENTES) VIVEN. ÉSTOS AGENTES INTERACTÚAN CON ÉSTE MUNDO DE FORMA ITERATIVA LLEVANDO A CABO DIFERENTES ACCIONES Y RECIBIENDO RECOMPENSAS.\\\n","SI LAS ACCIONES SON CORRECTAS -> RECOMPENSA POSITIVA.\\\n","CUANDO RECIBEN RE.POSITIVAS LOS AGENTES REFUERZAN ESAS ACCIONES BUENAS Y CUANDO RECIBEN RE.NEGATIVAS A NO HACERLAS TANTO."],"metadata":{"id":"B0Dz3ocM-pmg"}},{"cell_type":"markdown","source":["## BALANCE ENTRE EXPLORACIÓN Y EXPLOTACIÓN (PRINCIPAL PROBLEMA):\n","CUANDO TENEMOS AGENTES INTERACTUANDO CON UN MUNDO LLEVANDO A CABO ACCIONES, QUÉ QUEREMOS?\\\n","1. QUE EL AGENTE PRUEBE TODAS LAS ACCIONES POSIBLES A VER QUE PASA Ó\\\n","2. QUE UNA VEZ ENCUENTRE 1,2,3 ACCIONES BUENAS SÍMPLEMENTE HAGA ESAS Y YA ESTÁ. PERO\\\n","3. SI DE FORMA ALEATORIA EXPLORA OTRAS ACCIONES QUIZAS ENCUENTRE ALGUNAS QUE TODAVÍA SON MEJORES. (NO HABRÍA ENCONTRADO SI SE ENFOCA SÓLO EN LAS ACCIONES BUENAS QUE ENCONTRÓ PREVIAMENTE).\\\n","ÉSTE ES UN PROBLEMA QUE AÚN NO ESTÁ RESUELTO EN EL AxR.\\\n","-PREGUNTA QUE TENEMOS QUE HACERNOS: ¿QUÉ QUEREMOS? ¿AGENTES QUE EXPLOREN MÁS, AGENTEN QUE EXPLOTEN MÁS?"],"metadata":{"id":"2viFM8Knamdx"}},{"cell_type":"markdown","source":["VAMOS HACER EL TEMA DE LOS JUEGOS, VAMOS HACER AGENTES QUE DE FORMA AUTÓNOMA APRENDAR A JUGAR JUEGOS, IDEALMENTE MEJOR QUE UNA PERSONA."],"metadata":{"id":"NdGFulogcD97"}},{"cell_type":"markdown","metadata":{"id":"nLv53bfRVg20"},"source":["## Elementos del Aprendizaje por refuerzo\n","-Politica: Comportamiento del agente en cada momento\\\n","-Recompensa: recompensa numérica\\\n","-Función de valor: El valor de la recompensa a largo plazo\\\n","-Modelo del entorno: Comportamiento del entorno, sirve para planificar acciones.\n"]},{"cell_type":"markdown","source":["POLÍTICA: DEFINE COMO SE COMPORTA NUESTRO AGENTE, QUÉ ACCIONES VA A TOMAR EN FUNCIÓN DEL ESTADO DE ÉSTE MUNDO EN EL QUE VIVE. ESTA POLÍTICA PUEDE TENER MUCHAS FORMAS: LO PONGO AQUÍ EN UNA TABLA, PUEDE SER UN PROCESO DE BÚSQUEDA, PUEDE SER UNA RED NEURONAL O SÍMPLEMENTE LLEVAR A CABO ACCIONES ALEATORIAS. SON DIFERENTES POLÍTICAS QUE EN CIERTOS CASOS VAN A FUNCIONAR MEJOR Y EN OTROS CASOS PEOR. UN POCO EL OBJETIVO DE NUESTRO AGENTE ES QUE APRENDA UNA POLÍTICA ÓPTIMA.\\\n","RECOMPENSA: ES UN VALOR NUMÉRICO QUE EL AGENTE RECIBE DEL MUNDO EN EL QUE VIVE CUANDO LLEVA A CABO ACCIONES. LO UTILIZARÁ COMO OBJETIVO DE APRENDIZAJE, QUEREMOS QUE ÉSTOS AGENTES MAXIMICEN ÉSTA RECOMPENSA. LA POLÍTICA QUE MAXIMIZE ÉSTA RECOMPENSA SERÁ LA POLÍTICA ÓPTIMA.\\\n","FUNCIÓN DE VALOR: ES UNA FUNCIÓN QUE NOS VA A DECIR PARA CADA ESTADO EN EL QUE EL AGENTE SE ENCUENTRE, O CADA PAR DE ESTADO-ACCIÓN -> LE VAMOS ASIGNAR UN VALOR QUE NOS VA A DECIR LO BUENO QUE ES: SI NUESTRO AGENTE SE ENCUENTRA EN EL 'ESTADO S' Y DESDE EL 'ESTADO S' PUEDE TOMAR 3 ACCIONES (A1, A2 o A3). CADA UNA DE ÉSTAS ACCIONES TE LLEVARÁ A UN NUEVO ESTADO. SI SABEMOS EL VALOR DE ESE NUEVO ESTADO, PODEMOS COGER AQUELLA ACCIÓN QUE NOS DÉ UN VALOR MÁS GRANDE.\n","\n","$$\n","    Q_{n+1} = Q_n + \\alpha [R_n - Q_n]\n","$$\n","Qn+1 (NUEVO VALOR DE Q) -> EL VALOR DE Q SE VA IR ACTUALIZANDO DE MANERA ITERATIVA.\\\n","Qn -> VALOR VIEJO\\\n","alpha CONSTANTE ENTRE 0 Y 1. ES EL RATE DE APRENDIZAJE, LEARNING RATE.\\\n","Rn - Qn  -> DIFERENCIA ENTRE LA RECOMPENSA OBTENIDA Y EL VALOR DE Q QUE TENEMOS HASTA ESE MOMENTO.\n","\n","MODELO DEL ENTORNO: NUESTROS AGENTES VIVEN EN UN MUNDO QUE PUEDE SER EL MUNDO REAL O UN MUNDO VIRTUAL. NECESITAMOS MODELAR DE ALGUNA FORMA ÉSTE ENTORNO ASI SABREMOS QUÉ RECOMPENSA LE TENEMOS QUE DAR AL AGENTE, SI ES POSITIVA O NEGATIVA."],"metadata":{"id":"HK1jgPO8dH9e"}},{"cell_type":"markdown","metadata":{"id":"BDgdX8IFVg21"},"source":["## Aprendizaje por refuerzo aplicado a un juego 4 en Raya\n"]},{"cell_type":"markdown","source":["\\EXPLICACIÓN DEL ALGORITMO QUE VAMOS A UTILIZAR:\n","\n","-TENEMOS 2 AGENTES QUE VAN A JUGAR ENTRE SÍ AL TRES EN RAYA MUCHAS VECES (MUCHAS ITERACIONES).\\\n","-SE VAN A IR GUARDANDO TODOS LOS ESTADOS EN LOS QUE SE ENCUENTREN (CONFIGURACIONES DE LA TABLA TRES EN RAYA-> CADA UNO DE LOS ESTADOS) EN UNA LISTA, EN UNA TABLA.\\\n","-A CADA UNO DE ÉSTOS ESTADOS LE VA APLICAR UN VALOR. EN ÉSTE CASO NUESTRA FUNCIÓN DE VALOR ES LA TABLA DE CUATRO EN RAYA QUE ADEMÁS UTILIZAREMOS COMO POLÍTICA.\\\n","-LA IDEA ES QUE EN CADA TURNO NUESTRO AGENTE SABRÁ TODAS LAS POSIBLES ACCIONES QUE PUEDE LLEVAR A CABO-> TODOS LOS SITIOS EN LOS QUE PUEDE PONER UNA 'X' ó UNA 'O' EN FUNCIÓN DEL JUGADOR QUE SEA.\\\n","-Y COMO SABEMOS EL VALOR DE CADA UNO DE ÉSTOS ESTADOS, PODEMOS COGER SIEMPRE AQUELLA ACCIÓN QUE TENGA UN VALOR MÁS ALTO PORQUE EN PROMEDIO SERÁ EL QUE NOS DE MAYOR RECOMPENSA EL QUE NOS HAGA GANAR MÁS VECES AL FINAL LA PARTIDA.\n"],"metadata":{"id":"2Xbr2sv-h1cT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSqli4W-Vg23"},"outputs":[],"source":["import numpy as np\n","\n","\n","class Board():                                                                  #CLASE BOARD(), REPRESENTA LA CUADRÍCULA\n","    def __init__(self):\n","        self.state = np.zeros((4,4))              #INICIALIZAMOS UNA MATRIZ DE 3*3 CON CEROS QUE REPRESENTA LAS DIFERENTES POSICIONES.\n","                                                  #EL ESTADO LO REPRESENTA COMO UN TABLERO.\n","    def valid_moves(self):                        #ÉSTA FUNCIÓN DEVUELVE TODOS LOS POSIBLES MOVIMIENTOS QUE SON VÁLIDOS-> TODAS LAS CASILLAS QUE ESTÁN LIBRES EN CADA MOMENTO\n","        return [(i, j) for j in range(4) for i in range(4) if self.state[i, j] == 0]\n","\n","    def update(self, symbol, row, col):           #FUNCIÓN UPDATE -> VA ASIGNAR LA 'X' ó LA 'O', EN ÉSTE CASO SERÁN '1s' ó '-1s' A LA FILA Y COLUMNA EN CONCRETO\n","        if self.state[row, col] == 0:\n","            self.state[row, col] = symbol\n","        else:\n","            raise ValueError (\"movimiento ilegal !\")\n","\n","    def is_game_over(self):                       #FUNCIÓN PARA COMPROBAR SI YA HEMOS TERMINADO EL JUEGO\n","        # comprobar filas y columnas\n","        if (self.state.sum(axis=0) == 4).sum() >= 1 or (self.state.sum(axis=1) == 4).sum() >= 1:    #COMPRUEBA SI TENEMOS 3 unos EN HORIZONTAL O VERTICAL PARA TODAS LAS FILAS Y COLUMNAS\n","            return 1\n","        if (self.state.sum(axis=0) == -4).sum() >= 1 or (self.state.sum(axis=1) == -4).sum() >= 1:  #AQUÍ COMPRUEBA PARA 'O' QUE ES '-1s'\n","            return -1\n","        # comprobar diagonales                                                                      #COMPRUEBA EN DIAGONAL\n","        diag_sums = [\n","            sum([self.state[i, i] for i in range(4)]),\n","            sum([self.state[i, 4 - i - 1] for i in range(4)]),\n","        ]\n","        if diag_sums[0] == 4 or diag_sums[1] == 4:\n","            return 1\n","        if diag_sums[0] == -4 or diag_sums[1] == -4:\n","            return -1\n","        # empate                                    #SI LA COLUMNA ESTÁ LLENA PERO NO SE CUMPLEN NINGUNA DE LAS CONDICIONES ANTERIORES ES UN EMPATE\n","        if len(self.valid_moves()) == 0:\n","            return 0\n","        # seguir jugando                            #Y SINO SE SEGUIREMOS JUGANDO\n","        return None\n","\n","    def reset(self):                                #FUNCIÓN reset() VUELVE A PONER A CERO EL ESTADO PARA LA SIGUIENTE ITERACIÓN\n","        self.state = np.zeros((4,4))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_sH9G9bWVg25"},"outputs":[],"source":["from tqdm import tqdm\n","\n","class Game():                                                                   #CLASE GAME:SE ENCARGA DE LLEVAR A CABO TODAS LAS PARTIDAS\n","    def __init__(self, player1, player2):                                       #LE VAMOS A PASAR LOS DOS JUGADORES: player1 y player2\n","        player1.symbol = 1                                     #AL player1 LE ASIGNAMOS EL SÍMBOLO 1\n","        player2.symbol = -1                                    #AL player2 LE ASIGNAMOS EL SÍMBOLO -1\n","        self.players = [player1, player2]                      #TENEMOS \" JUGADORES\"\n","        self.board = Board()    #instancio un tablero, el objeto que he construido en la parte anterior\n","\n","\n","    def selfplay(self, rounds=100):                            #FUNCIÓN selfplay(), LE PASAMOS UN NÚMERO DE RONDAS, POR DEFECTO 100\n","                                                               #VA A COGER A LOS DOS PLAYERS (player1 y player2) QUE VAN A SER DOS AGENTES\n","                                                               #Y VAN A JUGAR EL UNO CONTRA EL OTRO TODAS LAS RONDAS. PARA CADA RONDA HASTA\n","                                                               #QUE LA PARTIDA ACABE-> QUE GANE EL JUGADOR1, QUE GANE EL JUGADOR2, O QUE HAYA UN EMPATE\n","        wins = [0, 0]\n","        for i in tqdm(range(1, rounds + 1)):\n","            self.board.reset()                                  #resetea el tablero al empezar el juego\n","            for player in self.players:\n","                player.reset()\n","            game_over = False                                   #pone en falso\n","\n","            while not game_over:                                #ÉSTE BUCLE SE VA ESTAR EJECUTANDO SIEMPRE QUE LA VARIABLE game_over ESTÉ A False\n","                for player in self.players:                     #Y TENEMOS POR LOS DIFERENTES JUGADORES QUE SON SÓLO DOS...\n","                    action = player.move(self.board)            #...COGIENDO UNA ACCIÓN. CADA player LLAMARÁ A SU FUNCIÓN move() Y NOS DARÁ UNA action.\n","                    self.board.update(player.symbol, action[0], action[1])      #Hay dos acciones. VAMOS HACER EL UPDATE() DE LA TABLA\n","\n","                    for player in self.players:\n","                        player.update(self.board)               #Y VAMOS IR GUARDANDO ÉSTE ESTADO. ÉSTE update() HACE QUE ÉSTE JUGADOR SE GUARDE EL ESTADO DE LA TABLA\n","                    if self.board.is_game_over() is not None:\n","                        game_over = True\n","                        break\n","                                                                #LOS DOS AGENTES SE IRÁN GUARDANDO TODOS ESTOS ESTADOS (while->break) HASTA QUE EL JUEGO ACABE\n","            self.reward()                                       #SALIMOS DEL BUCLE Y AQUÍ LE DAMOS LA RECOMPENSA\n","\n","            #ADEMÁS VOY A ESTAR GUARDÁNDOLAS PARA CADA PARTIDA QUIEN GANA PARA LUEGO COMPARAR ÉSTOS AGENTES:\n","            for ix, player in enumerate(self.players):          #saca el índice 0 o 1   ix\n","                if self.board.is_game_over() == player.symbol:\n","                    wins[ix] += 1                               #Va a aumentar sus victorias\n","        return wins\n","\n","\n","    def reward(self):                                           #FUNCIÓN reward() ASIGNA LA RECOMPENSA\n","        winner = self.board.is_game_over()                      #VEMOS QUIEN ES EL GANADOR (LA FUNCIÓN is_game_over() RETORNA 1,-1 ó 0)\n","        if winner == 0: # empate                                #SI HAY UN EMPATE LE DAMOS UNA RECOMPENSA DE...\n","            for player in self.players:\n","                player.reward(0.5)                              #...0.5 A AMBOS JUGADORES.\n","\n","        else: # le damos 1 recompensa al jugador que gana       #SI NO HAY EMPATE HA GANADO EL UNO O EL OTRO\n","            for player in self.players:\n","                if winner == player.symbol:\n","                    player.reward(1)                            #AL QUE HA GANADO LE DAMOS UNA RECOMPENSA DE 1\n","                else:\n","                    player.reward(0)                            #AL QUE HA PERDIDO LE DAMOS UNA RECOMPENSA DE 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KLg97K6Vg25"},"outputs":[],"source":["class Agent():                                                  #CLASE AGENTE(). VAMOS A VER CÓMO VA A APRENDER A JUGAR ÉSTE AGENTE DURANTE LAS PARTIDAS,\n","                                                                #DE CERO NO VA A SABER NADA.\n","    def __init__(self, alpha=0.5, prob_exp=0.5):\n","        self.value_function = {} # tabla con pares estado -> valor              #NUESTRO AGENTE VA A TENER ÉSTA FUNCIÓN DE VALOR, UN dic {} QUE EN PYTHON SE INTERPRETA COMO UNA TABLA\n","                                                                                #TABLA {}: EN UNA COLUMNA VAMOS A GUARDAR TODOS LOS ESTADOS QUE VEA EL AGENTE\n","                                                                                #DURANTE TODAS LAS PARTIDAS Y CADA UNO DE ELLOS LE VA A ASIGNAR UN VALOR\n","        self.alpha = alpha         # learning rate\n","        self.positions = []       # guardamos todas las posiciones de la partida.       #COMO SE LLEGA DE UN LUGAR A OTRO\n","        self.prob_exp = prob_exp   # probabilidad de explorar                   #AQUÍ TAMBIÉN TIENE UNA PROBABILIDAD DE EXPLORAR. LUEGO VEREMOS QUE NUESTRO AGENTE\n","                                                                                #VA A PROBAR ACCIONES ALEATORIAS, EN ESTE CASO CON UNA PROBABILIDAD DE 50%\n","                                                                                #VER ARRIBA: prob_exp=0.5-> ÉSTO ES ALGO QUE PODEMOS CONTROLAR\n","                                                                                #Y CON UNA PROB. DE 50% VA A COGER LA MEJOR ACCIÓN QUE HAYA ENCONTRADO HASTA\n","                                                                                #ESE MOMENTO...\n","\n","    def reset(self):\n","        self.positions = []\n","\n","    def move(self, board, explore=True):                                        #...ÉSTO ESTÁ DEFINIDO EN ÉSTA FUNCIÓN move()...\n","        valid_moves = board.valid_moves()                                       #VEMOS QUÉ POSICIONES SON VÁLIDAD EN LA TABLA (Ej.solo tengo 7 podiciones libres)\n","\n","        # exploracion                                                           #...TENEMOS LA FASE DE EXPLORACIÓN\n","        if explore and np.random.uniform(0, 1) < self.prob_exp:                 #SACAREMOS UN VALOR ALEATORIO ENTRE (0,1)->SI ES MÁS PEQUEÑO QUE LA\n","                                                                                #probabilidad de exploarción..\n","            # vamos a una posición aleatoria\n","            ix = np.random.choice(len(valid_moves))                             #..ELEGIMOS UNA ACCIÓN TOTALMENTE ALEATORIA ENTRE LAS VÁLIDAS (valid_moves)\n","            return valid_moves[ix]                                              #DEVUELVE ESE VALOR.\n","                                                                                #FASE EXPLORATORIA: EL AGENTE EXPLORA DE FORMA ALEATORIA ACCIONES, DE TAL\n","                                                                                #FORMA QUE PUEDE SER QUE ENCUENTRE ALGO MEJOR DE TODO LO QUE HA ENCONTRADO\n","                                                                                #HASTA EL MOMENTO.\n","\n","        # explotacion                                                           #FASE EXPLOTACIÓN: LAS ACCIONES QUE YO YA SÉ QUE SON BUENAS Y MALAS, VOY\n","                                                                                #A ELEGIR LA MEJOR QUE HAYA ENCONTRADO.\n","        # vamos a la posición con más valor\n","        max_value = -1000\n","        for row, col in valid_moves:                                            #PARA ELLO COGEMOS TODOS LOS MOVIMIENTOS VÁLIDOS\n","            next_board = board.state.copy()                                     #NOS CREAMOS UNA COPIA DE LA TABLA\n","            next_board[row, col] = self.symbol                                  #Y HACEMOS ESE MOVIMIENTO, VAMOS HACER TODOS ESOS MOVIMIENTOS VÁLIDOS  aquí le defines si es 1 o -1\n","            next_state = str(next_board.reshape(4*4))                           #COGER LA TABLA Y CONVERTIRLA EN UN string (cadena) [1 0 -1...]\n","            value = 0 if self.value_function.get(next_state) is None else self.value_function.get(next_state)   #Y PARA CADA UNO DE ÉSTOS MOVIMIENTOS VAMOS A\n","                                                                                #NUESTRA TABLA Y SACAR EL VALOR. SI NO LO ENCUENTRA EN LA TABLA ES UN ESTADO AL\n","                                                                                #QUE NUESTRO AGENTE NUNCA HA LLEGADO, POR TANTO LE PONGO UN VALOR DE CERO: value = 0\n","                                                                                #else self.value_function.get(next_state)->SI LO HA ENCONTRADO YA HA VISTO ESE ESTADO\n","                                                                                #EN ALGÚN MOMENTO Y POR TANTO YA TIENE UN VALOR->COGEMOS ESE VALOR\n","                                                                                #ÉSTO LO HAREMOS POR TODOS LOS MOVIMIENTOS VÁLIDOS..\n","\n","            if value >= max_value:                                              #..QUEDÁNDONOS CON AQUEL MOVIMIENTO QUE NOS DÉ UN VALOR MÁS GRANDE DE FORMA QUE CUANDO EL AGENTE\n","                                                                                #HAYA PROBADO TODOS LOS MOVIMIENTOS QUE PUEDA HACER, SABE CUÁL ES, EL QUE EN PROMEDIO\n","                                                                                #LE VA A LLEVAR SIEMPRE A GANAR CON MAYOR PROBABILIDAD -> ESE ES EL QUE ELIGE.\n","                                                                                #EXPLOTAR EL CONOCIMIENTO QUE TIENE PARA HACER EL MEJOR MOVIMIENTO.\n","                max_value = value\n","                best_row, best_col = row, col\n","        return best_row, best_col\n","\n","    def update(self, board):\n","        self.positions.append(str(board.state.reshape(4*4)))                    #está almacenando estados\n","\n","\n","    def reward(self, reward):                                                   #PERO CÓMO APRENDE: AL FINAL DE LA PARTIDA SE EMITEN ÉSTAS RECOMPENSAS\n","        # al final de la partida (cuando recibimos la recompensa)               #CADA AGENTE LLAMARÁ A ÉSTA FUNCIÓN reward(), AQUÍ RECIBIRÁ UN 0.5 SI HA\n","        # iteramos por tods los estados actualizando su valor en la tabla       #EMPATADO, 1 SI HA GANADO, 0 SI HA PERDIDO.\n","                                                                                #AQUÍ TENEMOS QUE ACTUALIZAR LA TABLA, IR POR TODAS LAS POSICIONES QUE HAYAMOS\n","                                                                                #VISTO DURANTE LA PARTIDA, Y EN LA TABLA TODAS AQUELLAS POR LAS QUE HAYA PASADO.\n","                                                                                #SI HA GANADO SUBIRLES EL VALOR Y SI HA PERDIDO BAJARLES EL VALOR.\n","                                                                                #DE FORMA QUE EN LA PRÓXIMA ITERACIÓN, SI POR LO QUE SEA LLEGA OTRA VEZ A ESE\n","                                                                                #ESTADO SABRÉ QUE YA GANÓ LA ANTERIOR VEZ Y HACE LA MISMA ACCIÓN. ó HACE 10 PARTIDAS\n","                                                                                #HE PERDIDO CON ÉSTA ACCIÓN, MEJOR OTRA ACCIÓN. SIGUE IMPLEMENTACIÓN.\n","\n","        for p in reversed(self.positions):                                      #ITERAMOS POR TODAS LAS POSICIONES QUE HA PASADO DURANTE LA PARTIDA\n","                                                                                #->EN ÉSTE BUCLE ITERAMOS DE FORMA INVERTIDA, EMPEZAMOS POR LA ÚLTIMA POSICIÓN\n","                                                                                #PARA LA CUAL TENEMOS LA RECOMPENSA FINAL(1 ó 0)\n","            if self.value_function.get(p) is None:\n","                self.value_function[p] = 0                                      #SI ESAS POSICIONES NO ESTABAN EN LA TABLA, LAS INICIALIZAMOS A CERO\n","            self.value_function[p] += self.alpha * (reward - self.value_function[p])    #SI ESTABAN EN LA TABLA APLICAMOS ÉSTA FUNCIÓN..\n","                                                                                #COGEMOS NUESTRO VALOR: self.value_function[p] Y LE SUMAMOS  self.alpha (LEARNING\n","                                                                                #RATE) COMO EN LAS REDES NEURONALES -> ES UN HIPERPARÁMETRO QUE NOSOTROS CONTROLAMOS\n","                                                                                #Y CALCULAMOS LA DIFERENCIA ENTRE LA RECOMPENSA Y EL VALOR QUE TENEMOS:(reward - self.value_function[p])\n","                                                                                #SI LA RECOMPENSA ES 1, ÉSTA DIFERENCIA SERÁ POSITIVA Y NUESTRO VALOR AUMENTARÁ.\n","                                                                                #SI HEMOS PERDIDO ÉSTA RECOMPENSA SERÁ NEGATIVA Y EL VALOR DESCENDERÁ PARA ESE ESTADO(POSICIÓN).\n","            reward = self.value_function[p]     #Siempre y cuando esos estados existan en la tabla    #->PARA LA PENÚLTIMA ACCIÓN LA RECOMPENSA ES EL VALOR DEL ÚLTIMO ESTADO.\n","                                                                                #ASÍ DE FORMA ITERATIVA: DESDE LA ÚLTIMA ACCIÓN QUE TOMASTE HASTA LA PRIMERA\n","                                                                                #IREMOS ACTUALIZANDO ÉSTOS VALORES: HACIA ARRIBA SI HEMOS GANADO, HACIA ABAJO SI\n","                                                                                #HEMOS PERDIDO."]},{"cell_type":"markdown","source":["## ENTRENAMIENTO DE DOS AGENTES:\n","agent1 CON UNA PROBABILIDAD DE EXPLORACIÓN DE 0.5\\\n","agent2 NO LE HE PUESTO NADA PERO COMO prob_exp = 0.5 YA ESTÁ DEFINIDA, VA A SER EXACTAMENTE IGUAL.\\\n","-VOY HACER QUE JUEGUEN 300000 PARTIDAS EL UNO CONTRA EL OTRO:\\\n","-SON 28:02 MINUTOS QUE JUEGAN (NO ESTÁ OPTIMIZADO, PODRÍA SER MÁS RÁPIDO).\\\n","-RECOMENDACIÓN: PROBAR DIFERENTES prob-exp, learning rates, diferentes números de partidas Y A EVALUAR QUE ES LO QUE SE CONSIGUE.\n"],"metadata":{"id":"TjNlOVsztPy7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jh9S5GThVg26","outputId":"71ae604d-c2cc-4ec1-80a2-35d185e3dce0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717494882943,"user_tz":240,"elapsed":2023425,"user":{"displayName":"Sandra Villca","userId":"10389606573201869473"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100000/100000 [33:43<00:00, 49.43it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["[34184, 28171]"]},"metadata":{},"execution_count":5}],"source":["agent1 = Agent(prob_exp=0.5)\n","agent2 = Agent()\n","\n","game = Game(agent1, agent2)\n","\n","game.selfplay(100000)"]},{"cell_type":"markdown","source":["EN ÉSTE CASO EL agent1 GANA 34184 PARTIDAS, EL agent2 28171.\\\n","LUEGO: SACAMOS LA FUNCIÓN DE VALOR DEL agent1: agent1.value_function.items()\\\n","LO CONVERTIMOS EN UNA TABLA CON PANDAS Y PODEMOS MOSTRAR.\n"],"metadata":{"id":"oQooBwpTuqYj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-BrBIuh-Vg26","outputId":"b21ac9a1-8481-432e-926d-7862e6034236","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"ok","timestamp":1717495084023,"user_tz":240,"elapsed":1777,"user":{"displayName":"Sandra Villca","userId":"10389606573201869473"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   estado  valor\n","0       [ 0.  0.  0.  1.  0.  0.  0.  1. -1. -1. -1.  ...    1.0\n","1       [ 0.  0.  0.  1.  0.  0.  0.  1. -1. -1.  0.  ...    1.0\n","2       [ 0.  0.  0.  1.  0.  0. -1.  1.  0. -1.  0.  ...    1.0\n","3       [ 0.  0.  0.  1.  0.  0.  0.  1. -1. -1.  0.  ...    1.0\n","4       [ 0.  0. -1.  1.  0.  0.  0.  1. -1. -1.  0.  ...    1.0\n","...                                                   ...    ...\n","540031  [-1. -1. -1.  1.  0.  0.  0. -1. -1.  0.  1.  ...    0.0\n","540032  [-1. -1. -1.  1.  0.  0.  0. -1. -1.  0.  1.  ...    0.0\n","540033  [-1. -1. -1.  1.  0.  0.  0. -1. -1.  0.  0.  ...    0.0\n","540034  [-1. -1. -1.  1.  0.  0.  0.  0. -1.  0.  0.  ...    0.0\n","540035  [-1. -1. -1.  1.  0.  0.  0.  0. -1.  0.  0.  ...    0.0\n","\n","[540036 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-b17f7969-f92c-41b3-b82c-47f086b0bf7c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>estado</th>\n","      <th>valor</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[ 0.  0.  0.  1.  0.  0.  0.  1. -1. -1. -1.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[ 0.  0.  0.  1.  0.  0.  0.  1. -1. -1.  0.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[ 0.  0.  0.  1.  0.  0. -1.  1.  0. -1.  0.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[ 0.  0.  0.  1.  0.  0.  0.  1. -1. -1.  0.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[ 0.  0. -1.  1.  0.  0.  0.  1. -1. -1.  0.  ...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>540031</th>\n","      <td>[-1. -1. -1.  1.  0.  0.  0. -1. -1.  0.  1.  ...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>540032</th>\n","      <td>[-1. -1. -1.  1.  0.  0.  0. -1. -1.  0.  1.  ...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>540033</th>\n","      <td>[-1. -1. -1.  1.  0.  0.  0. -1. -1.  0.  0.  ...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>540034</th>\n","      <td>[-1. -1. -1.  1.  0.  0.  0.  0. -1.  0.  0.  ...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>540035</th>\n","      <td>[-1. -1. -1.  1.  0.  0.  0.  0. -1.  0.  0.  ...</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>540036 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b17f7969-f92c-41b3-b82c-47f086b0bf7c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b17f7969-f92c-41b3-b82c-47f086b0bf7c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b17f7969-f92c-41b3-b82c-47f086b0bf7c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-d6f2bd12-d5c0-4ef4-8700-812904be1816\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d6f2bd12-d5c0-4ef4-8700-812904be1816')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-d6f2bd12-d5c0-4ef4-8700-812904be1816 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_69c810cf-475b-4e37-85a5-3be5ed332b54\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('tabla')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_69c810cf-475b-4e37-85a5-3be5ed332b54 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('tabla');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"tabla"}},"metadata":{},"execution_count":6}],"source":["import pandas as pd\n","\n","funcion_de_valor = sorted(agent1.value_function.items(), key=lambda kv: kv[1], reverse=True)\n","tabla = pd.DataFrame({'estado': [x[0] for x in funcion_de_valor], 'valor': [x[1] for x in funcion_de_valor]})\n","\n","tabla"]},{"cell_type":"markdown","source":["SE HA VISTO 540036 ESTADOS DIFERENTES DURANTE TODAS LAS PARTIDAS.\\\n","LA REPRESENTACIÓN DEL ESTADO DE LA PARTIDA ES: [0  0  0  1  0  0  0  1  .....1\n"," .....1]\\\n","DONDE ESTA EL jugador1 -> 1\\\n","DONDE ESTA EL jugador2 -> -1\\\n","CERO SI NO HAY NADIE\\\n","Y PARA CADA UNO DE ÉSTOS ESTADOS TENEMOS UN VALOR QUE VA DE 0 A 1.\\\n","VALOR 1 ES UN ESTADO EN EL QUE EL jugador1 GANA.\\\n","VALOR 0 ES UN ESTADO EN EL QUE EL jugador1 PIERDE.\\\n","\n","-AHORA CON ÉSTA TABLA: EN CADA TURNO NUESTRO agente PUEDE DECIR: CUÁLES SON LOS POSIBLES MOVIMIENTOS QUE PUEDO HACER -> PARA CADA UNO DE ELLOS VENDRÁ A ÉSTA TABLA Y TOMARÁ EL QUE TENGA EL VALOR MÁS GRANDE. SABE QUE SI LLEVA A CABO ESA ACCIÓN, EL ESTADO SIGUIENTE AL QUE VA, EN PROMEDIO LE VA HACER GANAR MÁS PARTIDAS DE LAS QUE LE VA A HACER PERDER PORQUE TIENEN UN VALOR MUY ALTO.\n"],"metadata":{"id":"IOxa-r7SzlgO"}},{"cell_type":"code","source":[],"metadata":{"id":"DWFRXULUcm12"},"execution_count":null,"outputs":[]}],"metadata":{"interpreter":{"hash":"bb9f406c0f70fca9801e60f2cbb7cd1ccff2ae2f74c58f513340bcf6cae5ecd0"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"provenance":[{"file_id":"https://github.com/juansensio/axr/blob/master/axr/00_intro.ipynb","timestamp":1715661008364}]}},"nbformat":4,"nbformat_minor":0}